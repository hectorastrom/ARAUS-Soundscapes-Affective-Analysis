{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Testing ARAUS\n",
    "\n",
    "We'll start by setting up our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 unique participants\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>fold_r</th>\n",
       "      <th>soundscape</th>\n",
       "      <th>masker</th>\n",
       "      <th>smr</th>\n",
       "      <th>stimulus_index</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>is_attention</th>\n",
       "      <th>pleasant</th>\n",
       "      <th>eventful</th>\n",
       "      <th>...</th>\n",
       "      <th>M04000_0_r</th>\n",
       "      <th>M05000_0_r</th>\n",
       "      <th>M06300_0_r</th>\n",
       "      <th>M08000_0_r</th>\n",
       "      <th>M10000_0_r</th>\n",
       "      <th>M12500_0_r</th>\n",
       "      <th>M16000_0_r</th>\n",
       "      <th>M20000_0_r</th>\n",
       "      <th>Leq_L_r</th>\n",
       "      <th>Leq_R_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARAUS_00009</td>\n",
       "      <td>4</td>\n",
       "      <td>R0087_segment_binaural_44100_1.wav</td>\n",
       "      <td>silence_00004.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>35.592</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>46.13</td>\n",
       "      <td>40.68</td>\n",
       "      <td>38.51</td>\n",
       "      <td>33.42</td>\n",
       "      <td>25.83</td>\n",
       "      <td>21.02</td>\n",
       "      <td>20.67</td>\n",
       "      <td>22.70</td>\n",
       "      <td>73.761966</td>\n",
       "      <td>75.353091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARAUS_00021</td>\n",
       "      <td>1</td>\n",
       "      <td>R0081_segment_binaural_44100_2.wav</td>\n",
       "      <td>traffic_00029.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>37.439</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>47.66</td>\n",
       "      <td>42.56</td>\n",
       "      <td>41.86</td>\n",
       "      <td>42.69</td>\n",
       "      <td>37.15</td>\n",
       "      <td>36.50</td>\n",
       "      <td>30.99</td>\n",
       "      <td>19.27</td>\n",
       "      <td>74.202644</td>\n",
       "      <td>73.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARAUS_00021</td>\n",
       "      <td>1</td>\n",
       "      <td>R0046_segment_binaural_44100_2.wav</td>\n",
       "      <td>bird_00047.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>37.833</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>43.73</td>\n",
       "      <td>39.67</td>\n",
       "      <td>35.29</td>\n",
       "      <td>33.46</td>\n",
       "      <td>27.51</td>\n",
       "      <td>19.27</td>\n",
       "      <td>18.67</td>\n",
       "      <td>12.37</td>\n",
       "      <td>67.246896</td>\n",
       "      <td>68.127026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARAUS_00021</td>\n",
       "      <td>1</td>\n",
       "      <td>R0080_segment_binaural_44100_2.wav</td>\n",
       "      <td>traffic_00006.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>33.782</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>44.57</td>\n",
       "      <td>43.30</td>\n",
       "      <td>43.81</td>\n",
       "      <td>36.82</td>\n",
       "      <td>31.24</td>\n",
       "      <td>28.05</td>\n",
       "      <td>23.03</td>\n",
       "      <td>17.66</td>\n",
       "      <td>67.395837</td>\n",
       "      <td>68.006605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARAUS_00021</td>\n",
       "      <td>1</td>\n",
       "      <td>R0115_segment_binaural_44100_1.wav</td>\n",
       "      <td>bird_00059.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>37.663</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>37.86</td>\n",
       "      <td>31.16</td>\n",
       "      <td>25.78</td>\n",
       "      <td>19.90</td>\n",
       "      <td>16.79</td>\n",
       "      <td>13.98</td>\n",
       "      <td>14.23</td>\n",
       "      <td>12.36</td>\n",
       "      <td>65.505416</td>\n",
       "      <td>66.575806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant  fold_r                          soundscape             masker  \\\n",
       "0  ARAUS_00009       4  R0087_segment_binaural_44100_1.wav  silence_00004.wav   \n",
       "1  ARAUS_00021       1  R0081_segment_binaural_44100_2.wav  traffic_00029.wav   \n",
       "2  ARAUS_00021       1  R0046_segment_binaural_44100_2.wav     bird_00047.wav   \n",
       "3  ARAUS_00021       1  R0080_segment_binaural_44100_2.wav  traffic_00006.wav   \n",
       "4  ARAUS_00021       1  R0115_segment_binaural_44100_1.wav     bird_00059.wav   \n",
       "\n",
       "   smr  stimulus_index  time_taken  is_attention  pleasant  eventful  ...  \\\n",
       "0    3              28      35.592             0         5         5  ...   \n",
       "1    0               4      37.439             0         5         5  ...   \n",
       "2    3               8      37.833             0         5         5  ...   \n",
       "3    3              11      33.782             0         5         5  ...   \n",
       "4    0              12      37.663             0         5         5  ...   \n",
       "\n",
       "   M04000_0_r  M05000_0_r  M06300_0_r  M08000_0_r  M10000_0_r  M12500_0_r  \\\n",
       "0       46.13       40.68       38.51       33.42       25.83       21.02   \n",
       "1       47.66       42.56       41.86       42.69       37.15       36.50   \n",
       "2       43.73       39.67       35.29       33.46       27.51       19.27   \n",
       "3       44.57       43.30       43.81       36.82       31.24       28.05   \n",
       "4       37.86       31.16       25.78       19.90       16.79       13.98   \n",
       "\n",
       "   M16000_0_r  M20000_0_r    Leq_L_r    Leq_R_r  \n",
       "0       20.67       22.70  73.761966  75.353091  \n",
       "1       30.99       19.27  74.202644  73.493800  \n",
       "2       18.67       12.37  67.246896  68.127026  \n",
       "3       23.03       17.66  67.395837  68.006605  \n",
       "4       14.23       12.36  65.505416  66.575806  \n",
       "\n",
       "[5 rows x 160 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"datasets/ARAUS_precleaned.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "print(len(data[\"participant\"].unique()), \"unique participants\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define all the values we will use for our analysis\n",
    "\n",
    "### DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data compressed from 32232 entries to 15729 entries in the merged table.\n",
      "Average grouping size: 2.049 soundscapes.\n",
      "Largest group merger count: 136.0 soundscapes\n",
      "\n",
      "Group size distribution:\n",
      "merge_count\n",
      "1      7696\n",
      "2      4600\n",
      "3      3048\n",
      "4        32\n",
      "5       112\n",
      "19       16\n",
      "20      129\n",
      "21       82\n",
      "22        7\n",
      "32        2\n",
      "136       5\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define which columns from cleaned_ARAUS.csv to use for the analysis\n",
    "# Format: {col_name in dataset: column name used for printing results}\n",
    "columns_of_interest = {\n",
    "    \"Savg_r\": \"Average Sharpness (acum)\",\n",
    "    \"Smax_r\": \"Peak Sharpness (acum)\",\n",
    "    \"Navg_r\": \"Average Loudness (sone)\",\n",
    "    \"Nmax_r\": \"Peak Loudness (sone)\",\n",
    "    \"Favg_r\": \"Average Fluctuation Strength (vacil)\",\n",
    "    \"Fmax_r\": \"Peak Fluctuation Strength (vacil)\",\n",
    "    \"Ravg_r\": \"Average Roughness (asper)\",\n",
    "    \"Rmax_r\": \"Peak Roughness (asper)\",\n",
    "    \"Tavg_r\": \"Average Tonality (tonality units)\",\n",
    "    \"Tmax_r\": \"Peak Tonality (tonality units)\",\n",
    "}\n",
    "# Define which columns from ARAUS_cleaned.csv to include in ARAUS_relevant.csv\n",
    "necessary_context = [\"participant\", \"soundscape\", \"masker\", \"time_taken\"]\n",
    "necessary_affective = [\"pleasant\", \"eventful\", \"chaotic\", \"vibrant\", \"uneventful\", \"calm\", \"annoying\", \"monotonous\"]\n",
    "relevant_data = data.loc[:, [*necessary_context, *necessary_affective, *list(columns_of_interest.keys())]]\n",
    "\n",
    "# Write ARAUS csv with only relevant columns to new csv\n",
    "relevant_data.to_csv(\"datasets/ARAUS_relevant.csv\", index=False)\n",
    "\n",
    "\n",
    "# TO GROUP OUR DATA:\n",
    "# Combine participant strings for each soundscape and masker\n",
    "# Count number of participants combined for each soundscape and masker\n",
    "# Mean every numerical column (time taken, all afffective ratings, all acoustic features)\n",
    "\n",
    "# Prime a new column for counting the rows grouped together\n",
    "relevant_data.insert(0, \"merge_count\", 1)\n",
    "\n",
    "# Create aggregations dictionary to define aggregation logic for each column\n",
    "aggregations = {\n",
    "    \"participant\": lambda x: ', '.join(x),\n",
    "    \"merge_count\": \"sum\",\n",
    "}\n",
    "\n",
    "# Select only the columns that contain numerical data (and can be averaged)\n",
    "numeric_cols = list(relevant_data.select_dtypes(include=['Float64', 'Int64']).columns)\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    if col_name not in aggregations:\n",
    "        aggregations[col_name] = \"mean\"\n",
    "\n",
    "merged_data = relevant_data.groupby([\"soundscape\", \"masker\"]).agg(aggregations)\n",
    "print(f\"Data compressed from {len(relevant_data)} entries to {len(merged_data)} entries in the merged table.\")\n",
    "print(\"Average grouping size:\", merged_data['merge_count'].mean().round(3), \"soundscapes.\")\n",
    "print(\"Largest group merger count: {size:.1f} soundscapes\".format(size=merged_data['merge_count'].max()))\n",
    "print(\"\\nGroup size distribution:\")\n",
    "print(merged_data[\"merge_count\"].value_counts().sort_index(), \"\\n\")\n",
    "# merged_data.sort_values(by=[\"merge_count\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10% of pleasant & vibrant soundscapes\n"
     ]
    }
   ],
   "source": [
    "class Group:\n",
    "    def __init__(self, remarkable: bool, short_hand: str, filter_cols: list, percentile: float, data: pd.DataFrame = merged_data):\n",
    "        self.remarkable = remarkable\n",
    "        self.short_hand = short_hand\n",
    "        self.filter_cols = filter_cols\n",
    "        self.percentile = percentile\n",
    "        self.data = data  # DataFrame to perform analysis on\n",
    "        self.descriptive_name = self.create_extended_name()\n",
    "        self.filtered_data = self.create_filtered_group()  # Store the filtered data as an attribute\n",
    "    \n",
    "    def create_extended_name(self):\n",
    "        \"\"\"Creates the descriptive name saying top/bottom {percentile} of *filter_cols.\"\"\"\n",
    "        prefix = \"top\" if self.percentile > 0.5 else \"tottom\"\n",
    "        readable_percentile = round(100 * (1 - self.percentile if self.percentile > 0.5 else self.percentile))\n",
    "        filters_description = \" & \".join(self.filter_cols)\n",
    "        return f\"{prefix} {readable_percentile}% of {filters_description} soundscapes\"\n",
    "\n",
    "    def create_filtered_group(self):\n",
    "        \"\"\"Filters the DataFrame based on provided filters and percentile, storing the result.\"\"\"\n",
    "        assert 0 <= self.percentile <= 1, \"Percentile must be between 0 and 1\"\n",
    "\n",
    "        ascend = self.percentile < 0.5\n",
    "        \n",
    "        # Sort and filter the DataFrame\n",
    "        group_data = self.data.sort_values(by=self.filter_cols + [\"merge_count\"], ascending=[ascend] * len(self.filter_cols) + [False])\n",
    "\n",
    "        slice_index = int(len(group_data) * self.percentile)\n",
    "        return group_data.iloc[:slice_index, :]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.descriptive_name\n",
    "\n",
    "# Remarkable groups\n",
    "# High percentile (.9) means the TOP 10% of the data\n",
    "# Order by which groups sorting will be prioritized (ex: if pleasant and vibrant listed, pleasant will be sorted first, then vibrant)\n",
    "remarkable_groups = [\n",
    "    Group(True, \"R_pleasant_90\", [\"pleasant\", \"vibrant\"], 0.9),\n",
    "    Group(True, \"R_eventful_90\", [\"eventful\"], 0.9),\n",
    "    Group(True, \"R_vibrant_90\", [\"vibrant\"], 0.9),\n",
    "]\n",
    "\n",
    "# Comparison groups\n",
    "# Low percentile (10) means BOTTOM 10% of the data\n",
    "comparison_groups = [\n",
    "    Group(False, \"C_pleasant_10\", [\"pleasant\"], 0.1),\n",
    "    Group(False, \"C_eventful_10\", [\"eventful\"], 0.1),\n",
    "    Group(False, \"C_vibrant_10\", [\"vibrant\"], 0.1),\n",
    "]\n",
    "\n",
    "# Printing for testing\n",
    "print(remarkable_groups[0])\n",
    "# print(remarkable_groups[0].filtered_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False, False], False]\n",
      "                                                    participant  merge_count  \\\n",
      "soundscape                         masker                                      \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav  ARAUS_00271            1   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav   ARAUS_00436            1   \n",
      "                                   water_00071.wav  ARAUS_00326            1   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav   ARAUS_00208            1   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav   ARAUS_00446            1   \n",
      "\n",
      "                                                    time_taken  pleasant  \\\n",
      "soundscape                         masker                                  \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav      35.953       5.0   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav       63.752       5.0   \n",
      "                                   water_00071.wav      51.469       5.0   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav       30.000       5.0   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav       79.551       5.0   \n",
      "\n",
      "                                                    eventful  chaotic  \\\n",
      "soundscape                         masker                               \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav       5.0      1.0   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav        5.0      3.0   \n",
      "                                   water_00071.wav       5.0      1.0   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav        5.0      1.0   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav        5.0      3.0   \n",
      "\n",
      "                                                    vibrant  uneventful  calm  \\\n",
      "soundscape                         masker                                       \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav      5.0         1.0   5.0   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav       5.0         1.0   3.0   \n",
      "                                   water_00071.wav      4.0         1.0   3.0   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav       4.0         2.0   4.0   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav       4.0         1.0   4.0   \n",
      "\n",
      "                                                    annoying  ...  Savg_r  \\\n",
      "soundscape                         masker                     ...           \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav       1.0  ...    1.43   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav        2.0  ...    1.25   \n",
      "                                   water_00071.wav       1.0  ...    1.91   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav        2.0  ...    1.21   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav        1.0  ...    1.20   \n",
      "\n",
      "                                                    Smax_r  Navg_r  Nmax_r  \\\n",
      "soundscape                         masker                                    \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav    1.80    16.2    19.8   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav     1.95    15.3    23.9   \n",
      "                                   water_00071.wav    2.13    21.8    28.4   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav     1.79    16.2    23.6   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav     2.50    17.4    38.8   \n",
      "\n",
      "                                                    Favg_r  Fmax_r  Ravg_r  \\\n",
      "soundscape                         masker                                    \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav  0.0242  0.1100  0.0292   \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav   0.0349  0.1210  0.0312   \n",
      "                                   water_00071.wav  0.0123  0.1160  0.0264   \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav   0.0607  0.1270  0.0324   \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav   0.0190  0.0905  0.0315   \n",
      "\n",
      "                                                    Rmax_r  Tavg_r  Tmax_r  \n",
      "soundscape                         masker                                   \n",
      "R0011_segment_binaural_44100_1.wav water_00057.wav  0.0590  0.1200   0.685  \n",
      "R0011_segment_binaural_44100_2.wav bird_00028.wav   0.0782  0.1530   0.921  \n",
      "                                   water_00071.wav  0.0612  0.0639   0.490  \n",
      "R0012_segment_binaural_44100_2.wav bird_00008.wav   0.1250  0.3060   1.270  \n",
      "R0018_segment_binaural_44100_2.wav bird_00055.wav   0.0799  0.5000   4.450  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'filter_vals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m col_interest_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns_of_interest\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# # Process all groups and store their statistics\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m remarkable_group_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremarkable_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_interest_titles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m comparison_group_data \u001b[38;5;241m=\u001b[39m process_groups(comparison_groups, data, col_interest_titles)\n\u001b[1;32m     82\u001b[0m remarkable_group_stats \u001b[38;5;241m=\u001b[39m process_statistics(remarkable_group_data)\n",
      "Cell \u001b[0;32mIn[94], line 44\u001b[0m, in \u001b[0;36mprocess_groups\u001b[0;34m(groups, data, columns)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# items are the key value pairs of each group in dict\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_name, group_info \u001b[38;5;129;01min\u001b[39;00m groups\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Create each group using the provided filters\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     single_group_datum \u001b[38;5;241m=\u001b[39m create_group(\n\u001b[0;32m---> 44\u001b[0m         data, group_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_cols\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mgroup_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter_vals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Calculate statistics for each group and store them\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     group_data[group_name] \u001b[38;5;241m=\u001b[39m single_group_datum[columns]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filter_vals'"
     ]
    }
   ],
   "source": [
    "def create_group(data : pd.DataFrame, filter_cols : list, percentile: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe of a group of data based on the provided filters and a percentile\n",
    "    \n",
    "    Args:\n",
    "        data: the main, large, dataframe to filter\n",
    "        filter_cols: a list of columns to filter the data by\n",
    "        percentile: a value between 0 and 1. Determines the cutoff point for the group\n",
    "\n",
    "    Percentiles above 50% are treated as \"high\" percentiles, and only the data above the percentile is kept\n",
    "    Conversely, percentiles below 50% are treated as \"low\" percentiles, and only the data below the percentile is kept\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame - A dataframe of the sorted and trimmed group data\n",
    "    \"\"\"\n",
    "    assert 0 <= percentile <= 1, \"Percentile must be between 0 and 1\"\n",
    "\n",
    "    ascend = False\n",
    "    if percentile < 0.5:\n",
    "        ascend = True\n",
    "\n",
    "    print([[ascend] * len(filter_cols), False])\n",
    "    # Prioritizes larger groups (more representative data, but only really has an effect when there's a single filter)\n",
    "    group_data = data.copy().sort_values(by=[*filter_cols, \"merge_count\"], ascending=[ascend] * len(filter_cols) + [False])\n",
    "    slice_index = int(len(group_data) * percentile)\n",
    "    group_data = group_data.iloc[:slice_index, :]\n",
    "\n",
    "    return group_data\n",
    "\n",
    "# print(create_group(merged_data, [\"pleasant\", \"eventful\"], 0.99).head())\n",
    "\n",
    "def process_groups(groups: dict, data: pd.DataFrame, columns: list) -> dict:\n",
    "    \"\"\"\n",
    "    Creates groups from list of filters\n",
    "\n",
    "    Returns:\n",
    "      dict - \"group names\" : [relevant group data]\n",
    "    \"\"\"\n",
    "    group_data = {}\n",
    "    # items are the key value pairs of each group in dict\n",
    "    for group_name, group_info in groups.items():\n",
    "        # Create each group using the provided filters\n",
    "        single_group_datum = create_group(\n",
    "            data, group_info[\"filter_cols\"], group_info[\"filter_vals\"]\n",
    "        )\n",
    "        # Calculate statistics for each group and store them\n",
    "        group_data[group_name] = single_group_datum[columns]\n",
    "\n",
    "        # Write group data to csv\n",
    "        if group_name[0] == \"R\":\n",
    "            path = f\"datasets/remarkable_groups/{group_name}.csv\"\n",
    "        else:\n",
    "            path = f\"datasets/comparison_groups/{group_name}.csv\"\n",
    "\n",
    "        single_group_datum[necessary_context + columns].to_csv(path, index=False)\n",
    "    return group_data\n",
    "\n",
    "\n",
    "def calculate_statistics(group: pd.DataFrame):\n",
    "    \"\"\"Calculate mean, standard deviation and size all columns in a group\"\"\"\n",
    "    return {\n",
    "        \"mean\": group.mean(),\n",
    "        \"standard deviation\": group.std(),\n",
    "        \"group size\": len(group),\n",
    "        \"variance\": group.var(),\n",
    "        # Add more statistics as needed\n",
    "    }\n",
    "\n",
    "def process_statistics(group_data: dict):\n",
    "    \"\"\"Calculate statistics for all groups in a dictionary\"\"\"\n",
    "    group_stats = {}\n",
    "    for group_name, group in group_data.items():\n",
    "        group_stats[group_name] = calculate_statistics(group)\n",
    "    return group_stats\n",
    "\n",
    "\n",
    "col_interest_titles = list(columns_of_interest.keys())\n",
    "\n",
    "# # Process all groups and store their statistics\n",
    "remarkable_group_data = process_groups(remarkable_groups, data, col_interest_titles)\n",
    "comparison_group_data = process_groups(comparison_groups, data, col_interest_titles)\n",
    "remarkable_group_stats = process_statistics(remarkable_group_data)\n",
    "comparison_group_stats = process_statistics(comparison_group_data)\n",
    "\n",
    "# print(remarkable_group_stats)\n",
    "# print(comparison_group_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_group(group_data: dict, group_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Selects a group from the given group_data dictionary (which contains at least 1 group) and returns \n",
    "    a dictionary containing only the selected group's statistics or data.\n",
    "\n",
    "    Works taking in both group_data and group_stats dictionaries.\n",
    "    \"\"\"\n",
    "    return {group_name: group_data[group_name]}\n",
    "\n",
    "def print_group_stats(group_stats: dict, columns_of_interest: dict, printable_stats: list = [\"mean\", \"standard deviation\", \"group size\"]):\n",
    "    \"\"\"\n",
    "    Prints the statistics of each group in a formatted manner, using the labels from group_stats.\n",
    "    \"\"\"\n",
    "    for group_name, stats in group_stats.items():\n",
    "        group_size = stats[\"group size\"]\n",
    "        print(f\"### Group: {group_name} - size: {group_size}\")\n",
    "        for col_key, label in columns_of_interest.items():\n",
    "            for stat_key, stat_value in stats.items():\n",
    "                if stat_key in printable_stats:\n",
    "                    # Check if stat_value is a DataFrame or series\n",
    "                    if isinstance(stat_value, (pd.DataFrame, pd.Series)):\n",
    "                        formatted_label = f\"{stat_key.capitalize()} {label}\"\n",
    "                        print(f\"    - **{formatted_label}**: {stat_value[col_key]:.4f}\")\n",
    "                    else:  # For non-DataFrame/Series statistics (like 'size')\n",
    "                        if stat_key != \"group size\":\n",
    "                            formatted_label = f\"{stat_key.capitalize()} of {group_name}\"\n",
    "                            print(f\"    - **{formatted_label}**: {stat_value}\")\n",
    "            print()\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "def significance_test(one_data: pd.DataFrame, two_data: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the t-statistic and p-value for the two given groups.\n",
    "\n",
    "    Args:\n",
    "        one_data (pd.DataFrame): Dataframe containing data for the first group (with only relevant columns).\n",
    "        two_data (pd.DataFrame): Dataframe containing data for the second group.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of two dictionaries containing t-statistics and p-values for each column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing dictionaries for results\n",
    "    t_statistics = {}\n",
    "    p_values = {}\n",
    "\n",
    "    # Iterating over each statistic and calculating t-statistic and p-value\n",
    "    for stat_name in one_data.columns:\n",
    "        t_statistic, p_value = ttest_ind(\n",
    "            one_data[stat_name], \n",
    "            two_data[stat_name], \n",
    "            equal_var=False, \n",
    "            nan_policy=\"omit\"\n",
    "        )\n",
    "        t_statistics[stat_name] = t_statistic\n",
    "        p_values[stat_name] = p_value\n",
    "\n",
    "    return t_statistics, p_values\n",
    "\n",
    "\n",
    "def verbose_compare_groups(\n",
    "    group_one_data: dict, \n",
    "    group_two_data: dict,\n",
    "    columns_of_interest: dict, \n",
    "    file,\n",
    "    test_p: float = 0.01, \n",
    "    include_insignificant: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares two groups, prints the comparison of their means, and calculates the t-test for significant difference in means.\n",
    "\n",
    "    Args:\n",
    "        group_one_data (dict): Dictionary containing statistics for the first group.\n",
    "        group_two_data (dict): Dictionary containing statistics for the second group.\n",
    "        columns_of_interest (dict): Dictionary mapping data column keys to their descriptive labels.\n",
    "        file (Python I/O file object): Open file that can be written to \n",
    "        test_p (float): Value for testing significance of t-tests (typically 0.05 or 0.01)\n",
    "        include_insignificant (bool): Whether or not to include insignificant t-tests in the output\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve group names\n",
    "    group_one_name, group_two_name = list(group_one_data.keys())[0], list(group_two_data.keys())[0]\n",
    "    one_data, two_data = list(group_one_data.values())[0], list(group_two_data.values())[0]\n",
    "\n",
    "    file.write(f\"## COMPARISON BETWEEN {group_one_name} AND {group_two_name} GROUPS\\n\\n\")\n",
    "\n",
    "    means_one = calculate_statistics(one_data)['mean']\n",
    "    means_two = calculate_statistics(two_data)['mean']\n",
    "\n",
    "    # Perform t-test and compare means\n",
    "    t_stats, p_values = significance_test(one_data, two_data)\n",
    "\n",
    "    underscore_counter = 0\n",
    "\n",
    "    # Write comparison of means and summary of t-tests\n",
    "    for col_key, label in columns_of_interest.items():\n",
    "        group_one_stat = means_one[col_key]\n",
    "        group_two_stat = means_two[col_key]\n",
    "\n",
    "        higher_lower = \"**HIGHER**\" if group_one_stat > group_two_stat else \"**LOWER**\"\n",
    "        inverse_higher_lower = \"**LOWER**\" if higher_lower == \"**HIGHER**\" else \"**HIGHER**\"\n",
    "\n",
    "        file.write(f\"### PARAMETER: {label}\\n\")\n",
    "        file.write(f\"- *{group_one_name}* MEAN: {group_one_stat:.4f} - {higher_lower}\\n\")\n",
    "        file.write(f\"- *{group_two_name}* MEAN: {group_two_stat:.4f} - {inverse_higher_lower}\\n\")\n",
    "\n",
    "        significance = \"**STATISTICALLY SIGNIFICANT**\" if p_values[col_key] < test_p else \"NOT A STATISTICALLY SIGNIFICANT\"\n",
    "        if include_insignificant or p_values[col_key] < test_p:\n",
    "            file.write(f\"> {significance} DIFFERENCE WITH P={test_p}: p-value: {p_values[col_key]:.4f}, t-value: {t_stats[col_key]:.4f}\\n\\n\")\n",
    "\n",
    "        underscore_counter += 1\n",
    "        if underscore_counter % 2 == 0 and underscore_counter != 10:\n",
    "            file.write(\"-------------------\\n\")\n",
    "\n",
    "\n",
    "# Compare all remarkable and comparison groups, output to total_comparisons.md file\n",
    "with open(\"t-test-outputs/total_comparisons.md\", \"w\") as file:\n",
    "    # Generate table of contents\n",
    "    file.write(\"# Table of Contents\\n\")\n",
    "    for r_group in remarkable_group_data:\n",
    "        for c_group in comparison_group_data:\n",
    "            file.write(f\"- [{r_group} vs. {c_group}](#comparison-between-{r_group.lower()}-and-{c_group.lower()}-groups)\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    for r_group in remarkable_group_data:\n",
    "        for c_group in comparison_group_data:\n",
    "            verbose_compare_groups(\n",
    "                select_group(remarkable_group_data, r_group),\n",
    "                select_group(comparison_group_data, c_group),\n",
    "                columns_of_interest,\n",
    "                file\n",
    "            )\n",
    "        file.write(\"______________________________________</br></br></br></br>\\n\\n\")\n",
    "\n",
    "\n",
    "# Compare corresponding remarkable and comparison groups (pleasant vs. pleasant, vibrant vs. vibrant, etc.)\n",
    "with open(\"t-test-outputs/corresponding_comparisons.md\", \"w\") as file:\n",
    "    file.write(\"# Table of Contents\\n\")\n",
    "    for r_group, c_group in zip(remarkable_group_data, comparison_group_data):\n",
    "        file.write(f\"- [{r_group} vs. {c_group}](#comparison-between-{r_group.lower()}-and-{c_group.lower()}-groups)\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    for r_group, c_group in zip(remarkable_group_data, comparison_group_data):\n",
    "        verbose_compare_groups(\n",
    "            select_group(remarkable_group_data, r_group),\n",
    "            select_group(comparison_group_data, c_group),\n",
    "            columns_of_interest,\n",
    "            file\n",
    "        )\n",
    "        file.write(\"______________________________________</br></br></br></br>\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
